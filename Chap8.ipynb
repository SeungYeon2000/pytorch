{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9RQ67MUCHrxNBdin+yCq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09d740b3222842dbb071def1685adff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3ab8a939bbf4a88a24a422a7f1ef858",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ed93491bd03846c88565a35d724c2c53",
              "IPY_MODEL_fb347e3d19d54747bc3df6013870ba45"
            ]
          }
        },
        "e3ab8a939bbf4a88a24a422a7f1ef858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed93491bd03846c88565a35d724c2c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad8856536a7d4d59ab521b539731626d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4f8c38102c649cea25a9f22748bf07d"
          }
        },
        "fb347e3d19d54747bc3df6013870ba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d3bd4f6f3b14021a649b7524ce5c1dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:07&lt;00:00, 24080000.45it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d782ec6cee7840a49dd4a726680a7678"
          }
        },
        "ad8856536a7d4d59ab521b539731626d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4f8c38102c649cea25a9f22748bf07d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d3bd4f6f3b14021a649b7524ce5c1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d782ec6cee7840a49dd4a726680a7678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeungYeon2000/pytorch/blob/main/Chap8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4XUYmSP5E6o"
      },
      "source": [
        "# **Chap8. Using Convolutions to generalize**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bwsBqnh64DR"
      },
      "source": [
        "from CH7., 특성 일반화 < training set 기억\n",
        "\n",
        "-> 1. 변수가 너무 많음\n",
        "\n",
        "1은 cropping images로도 해결 어려워서 **convolution**이 필요!\n",
        "\n",
        "-> 2. 위치 독립성 X (일반화 어렵)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7vUazkG9zD9"
      },
      "source": [
        "# 8.1 The case for convolutions\n",
        "\n",
        "convolutions의 기능 : 위치 deliver, translation invariance(input달라도 output같음.)\n",
        "\n",
        "멀리 떨어진 pixel보다는 근처 pixel과의 weighted sum을 계산함. = weight 행렬 만들기(결과 특징 하나당, 결과 픽셀 위치당).+중심 픽셀로부터 특정 거리 너머의 모든 weights는 0.\n",
        "\n",
        "=> \" a linear operation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlcyhVtK_Kf7"
      },
      "source": [
        "# 8.1 What convolutions do\n",
        "training 중에 network가 업데이트 되는 동안 translation invariant위해 픽셀을 같은 값으로 초기화시켜야. & tied weights(언어모델 향상)가 같도록 해야.\n",
        "\n",
        "=> weights가 이웃 weights와 작동한다 (local 패턴에 반응해서, 위치와 관계없이 local 패턴 인식 가능)\n",
        "\n",
        "---\n",
        "결론_*convolution* : 이용가능하고 local하며 translation-invariant한 linear operation이다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8uwm6uVIGu2"
      },
      "source": [
        "convoluiton (2D image) - scalar * weight 행렬 = kernel\n",
        "\n",
        "결과 이미지의 요소 계산 가능.(bias 없이)\n",
        "\n",
        "input image의 위치에서 kernel을 \"translate\"할 수 있음.\n",
        "\n",
        "output image는 모든 input 위치에서 kernel을 translate하는 것과 weighted sum을 수행함으로써 만들어진다.\n",
        "\n",
        "weight는 랜덤으로 초기화되고 역전파에 의해 업데이트됨 + 같은 kernel이 전체 이미지에서 재사용됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ55szibbUeQ"
      },
      "source": [
        "Convolutions으로 인해\n",
        "- local operations on neighborhoods\n",
        "- Translation invariance\n",
        "- Models with 훨씬 적은 parameters (pixel의 갯수가 아니라 convolution kernel size/얼마나 많은 convolution filters or output channels에 의해)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDe0tl6XbjZa"
      },
      "source": [
        "# 8.2 Convolutions in action\n",
        "torch.nn module -> 1,2,3 dimensions (nn.Con1d:시간/nn.Con2d:이미지/nn.Con3d:volumes 나 비디오)\n",
        "\n",
        "CIFAR-10이니까 image(nn.Con2d)로. 여기에 우리가 주는 arguments는 input features나 channels의 갯수, output features의 갯수, 커널 사이즈. \n",
        "\n",
        "(output에서 channels가 많을수록 network의 용량도 커진다.)\n",
        "\n",
        "kernel_size=3은 kernel_size=(3,3) 대신 쓸 수 있는 단축키."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MRMkydjGveV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a6a225-371e-4427-e0c2-f7c624d56166"
      },
      "source": [
        "#In[1]:\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc5eb11dab0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lagp1M174vCR"
      },
      "source": [
        "#In [2]:\n",
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHMlra9IGths",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "09d740b3222842dbb071def1685adff0",
            "e3ab8a939bbf4a88a24a422a7f1ef858",
            "ed93491bd03846c88565a35d724c2c53",
            "fb347e3d19d54747bc3df6013870ba45",
            "ad8856536a7d4d59ab521b539731626d",
            "d4f8c38102c649cea25a9f22748bf07d",
            "9d3bd4f6f3b14021a649b7524ce5c1dd",
            "d782ec6cee7840a49dd4a726680a7678"
          ]
        },
        "outputId": "6a3a6d87-1357-46dd-8b03-fe3e24d1ddd1"
      },
      "source": [
        "#In [3]:\n",
        "from torchvision import datasets, transforms\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09d740b3222842dbb071def1685adff0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnTXCXaoen6C",
        "outputId": "9d0401b1-f647-4a41-8d2b-20d673207f4d"
      },
      "source": [
        "#In [4]:\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21D0U5DaevHV"
      },
      "source": [
        "#In [5]:\n",
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rB1IM88exvm"
      },
      "source": [
        "#In [6]:\n",
        "connected_model = nn.Sequential(\n",
        "            nn.Linear(3072, 1024),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQDVcacwezj-",
        "outputId": "c7f1516e-d452-45de-92c1-83bdc64633b0"
      },
      "source": [
        "#In [7]:\n",
        "numel_list = [p.numel()\n",
        "              for p in connected_model.parameters()\n",
        "              if p.requires_grad == True]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM1gmMA0e6Kn"
      },
      "source": [
        "#In [8]:\n",
        "first_model = nn.Sequential(\n",
        "                nn.Linear(3072, 512),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(512, 2),\n",
        "                nn.LogSoftmax(dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeQA-UXge-vO",
        "outputId": "5952c361-c5fb-4b0f-dcf4-0e874bbe2eb4"
      },
      "source": [
        "#In [9]:\n",
        "numel_list = [p.numel() for p in first_model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1574402, [1572864, 512, 1024, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slmdl-QMfBI0",
        "outputId": "19c80d60-08db-4e5a-c40a-ad64a63521a8"
      },
      "source": [
        "#In [10]:\n",
        "linear = nn.Linear(3072, 1024)\n",
        "\n",
        "linear.weight.shape, linear.bias.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 3072]), torch.Size([1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00YoKzBJfFHf",
        "outputId": "92a34b4b-3a21-4021-ab0c-27989109ea24"
      },
      "source": [
        "#In[11]:\n",
        "conv = nn.Conv2d(3,16,kernel_size=3)    #kernel_size=3 대신 쓸 수 있는 것? -> kernel_size=(3,3)\n",
        "conv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYTaAw8KpsTC"
      },
      "source": [
        "kernel size가 3*3이니까 weight tensor는 3*3 part로 예상"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HGG4vl5fKmy",
        "outputId": "562c2b5a-a183-4486-ddc5-5ed6a54dc9ea"
      },
      "source": [
        "#In[12]:\n",
        "conv.weight.shape, conv.bias.shape    # weights가 전체 이미지에 최적화된 local patterns를 찾는 더 작은 모델을 갖게 됨."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTZcqtCPqpBq",
        "outputId": "31deeb65-bca4-4dda-a5ee-5e463db564ae"
      },
      "source": [
        "#In[13]:\n",
        "img,_ = cifar2[0]\n",
        "output = conv(img.unsqueeze(0))   #0번째 batch 차원을 더함. (nn.Conv2d는 input tensor 형식이 B*C*H*W라서.)\n",
        "img.unsqueeze(0).shape, output.shape  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "VXYEl3uBqLz1",
        "outputId": "4a45b0be-d86b-44ec-a9d4-eb0a47bdb016"
      },
      "source": [
        "#In[15]:\n",
        "plt.imshow(output[0,0].detach(),cmap='gray')\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ANQgyWJreXh"
      },
      "source": [
        "# 8.2.1 Padding the boundary\n",
        "output이 input보다 작다는 사실은 이미지 경계에서 뭘 할지에 관한 부작용.\n",
        "\n",
        "- convolution kernel as a weighted sum of pixels in a 3*3 neighborhood : 모든 방향에 neighbors가 있어야지만 가능. ex) i00의 경우 왼쪽면에 존재 X\n",
        "- Pytorch는 width-kernel_width+1을 퉁해 convolution kernel을 slide함. (초기설정)\n",
        "- 홀수 kernel의 경우 convolution kernel/2 만큼 작은 kernel의 width를 갖게 됨.\n",
        "*** 그러나 (Zero)PADDING을 하게 될 경우 i00번째 요소라도 추가 set를 가지기 때문에 kernel 유실 없음., 다만 padding을 하더라도 weight나 bias의 크기는 변화 없음!\n",
        "\n",
        "  [padding하는 이유]\n",
        "\n",
        "1. convolution과 이미지 사이즈 변화의 문제를 분리시키고 \n",
        "2. 더욱 정교한 구조를 갖게 된다(ex. skip connections, U-Nets), 몇 번의 컨볼루션 전후에 텐서를 추가하거나 차이를 취할 수 있도록 호환 가능한 크기로."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7WSAMJdrTcn",
        "outputId": "42b7e965-e206-4c9e-d5fd-b879babfd47b"
      },
      "source": [
        "#In[16]:\n",
        "conv=nn.Conv2d(3,1,kernel_size=3,padding=1)\n",
        "output=conv(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWQZef9FvPpB"
      },
      "source": [
        "#8.2.2 Detecting features with convolutions\n",
        "1. bias 줄이기 (혼재 줄임).\n",
        "2. weights를 상수값으로 두어 결과의 각 픽셀이 주변의 평균값을 가지도록."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Xwjg56wH0w"
      },
      "source": [
        "#In[17]:\n",
        "with torch.no_grad():\n",
        "  conv.bias.zero_()\n",
        "\n",
        "with torch.no_grad():\n",
        "  conv.weight.fill_(1.0/9.0)    #weight 설정\n",
        "# conv.weight.one_()는 주변 픽셀의 합을 나타냄."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "pcPAfuTdxHdE",
        "outputId": "2967c0da-86db-450b-dc3b-3f1991269f9e"
      },
      "source": [
        "#In[18]:\n",
        "output=conv(img.unsqueeze(0))\n",
        "plt.imshow(output[0,0].detach(),cmap='ocean')\n",
        "plt.show\n",
        "# 블러 처리된 (좀 더 스무스하게 연관되어 바뀐) 이미지 등장."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF30wcV7xs0m"
      },
      "source": [
        "# **참고**\n",
        "underbar(_)가 있는 경우 : in_place (기존 tensor에서. 새로운 tensor 생성되지 않음.)\n",
        "\n",
        "underbar(_)가 없는 경우 : 새로운 tensor를 만들어냄.\n",
        "\n",
        "detach() : 기존 Tensor에서 gradient 전파가 안되는 텐서 생성,단 storage를 공유하기에 detach로 생성한 Tensor가 변경되면 원본 Tensor도 똑같이 변함. \n",
        "\n",
        "plt.imshow()에 cmap= 인자를 지정하면 이미지에 적용되는 colormap을 변경할 수 있음. 지정하지 않으면 무관한 색상이 등장."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yJblBjzzOF_"
      },
      "source": [
        "#In[19]:\n",
        "conv = nn.Conv2d(3,1,kernel_size=3,padding=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  conv.weight[:] = torch.tensor(\n",
        "      [[-1.0,0.0,1.0],\n",
        "       [-1.0,0.0,1.0],\n",
        "       [-1.0,0.0,1.0]]\n",
        "  )\n",
        "  conv.bias.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3WtydbwBz7"
      },
      "source": [
        "kernel이 다른 강도의 두 인접한 지역 사이 수직 경계에 적용되면 o22 = 높은 값.\n",
        "\n",
        "kernel이 균일한 강도로 적용되면 o22 = 0. \n",
        "\n",
        "=>(edge-detection kernel), kernel은 두 수평으로 인접한 지역 사이 수직 구간을 강조함.\n",
        "\n",
        "convolution kernel은 수직 가장자리를 향상시킨다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDTW2HGv19W2"
      },
      "source": [
        "# 8.2.3 Looking further with depth and pooling\n",
        "CIFAR-10 images는 작지만, objects는 여전히 여러 픽셀들이 교차하고 있다. 이에 따른 해결 방안\n",
        "1. From large to small : Downsampling : (이미지를 반으로 스케일링하는 것) = (인풋으로 네 개의 이웃 픽셀을 취하고 한 픽셀을 아웃풋으로 두는 것)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1lLI62V3ipA"
      },
      "source": [
        "1. Downsampling\n",
        ": 네 개의 픽셀 평균화 \"average pooling\"\n",
        "\n",
        ": 네 개의 픽셀 중 가장 큰 값 취하기 \"max pooling\"\n",
        ", nn.MaxPool2d module로 제공됨.\n",
        "\n",
        ": n 번째 픽셀만 계산되는 strided convolution 하기\n",
        "\n",
        "2. Combining convolutions and downsampling for great good\n",
        ": "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhltz-t84JEq",
        "outputId": "9fe74478-60f8-4e48-f354-1e7622ece3c7"
      },
      "source": [
        "#In[21]:\n",
        "pool = nn.MaxPool2d(2)      # 주어진 image를 반만큼(1/2) downsample한 거니까 2로 써줌.\n",
        "output = pool(img.unsqueeze(0))\n",
        "\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjoJhAi6jJo"
      },
      "source": [
        "1차 - 첫 번째 커널 세트는 small neighborhoods로 작동, low level features\n",
        "\n",
        "2차 - 두 번째 커널 세트는 좀 더 넓은 neighborhoods까지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO9nme4l7PLd"
      },
      "source": [
        "# **8.2.4 Putting it all together for our network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u1y3Y2D4TOi"
      },
      "source": [
        "#In[22]:\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3,16,kernel_size=3,padding=1),    #3RGB channels -> 16, 낮은 단계로 구별 가능\n",
        "    nn.Tanh(),                                  \n",
        "    nn.MaxPool2d(2),                            # 그 결과 16-channel 32*32 image가 16-channel 16*16 image\n",
        "    nn.Conv2d(16,8,kernel_size=3,padding=1),    # 그 결과 8-channel 16*16 output (더 높은 레벨의 feature)\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2)                             # 그 결과 8-channel 8*8 output\n",
        "\n",
        ")\n",
        "# 8-channel 8*8 image를 1D 벡터로 바꾸고 fully connected layers로 network를 완성하는 것이 우리의 초기목표."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI1u9ofU71Dh"
      },
      "source": [
        "#In [23]:\n",
        "model = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2),\n",
        "            # ... Something is missing : 8-channel 8*8 image -> 512-element로 바꿔줘야 함.(마지막 nn.MaxPool2d 결과에서 view호출함으로써 가능.)\n",
        "            nn.Linear(8 * 8 * 8, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 2))\n",
        "# linear layer의 크기는 MaxPool2d의 예상되는 결과 사이즈에 달려있다. 8*8*8=512."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLNph2Wy9r0u",
        "outputId": "09ca26bb-12b3-4687-b97f-5eb1fb4b9191"
      },
      "source": [
        "#In [24]:\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list\n",
        "# 모델의 용량을 늘리려면 convolution layers로 아웃풋 채널 수를 늘려야 한다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "265nCEoG-M_J",
        "outputId": "84fd7c9c-de3c-46bf-cda9-9880bcb034b7"
      },
      "source": [
        "#In [25]:\n",
        "model(img.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0e5b244e0a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#In [25]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_p_JRi1Ccbm"
      },
      "source": [
        "#In [26]:\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvj4s68gDWUO"
      },
      "source": [
        "# 8.3 Subclassing nn.Module\n",
        "우리만의 nn.Module subclasses를 만들어서 미리 만들어진 것들이나 nn.Sequential처럼 사용할 수 있도록 함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QTa0fX6DQ6C",
        "outputId": "b2019711-7e15-4a50-96ed-3d9aebd09e6e"
      },
      "source": [
        "#In [27]:\n",
        "model = Net()\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbhsHQ4nEVuj"
      },
      "source": [
        "#In [28]:\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnzbt-LnEYGJ",
        "outputId": "48ec4f45-7574-4e61-c940-b606995ddcb6"
      },
      "source": [
        "#In [29]:\n",
        "model = Net()\n",
        "model(img.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0157,  0.1143]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUwcsrCXEaYl"
      },
      "source": [
        "#In [30]:\n",
        "import datetime  # python에 포함된 datatime 사용\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # 0에서 시작 아니고 1부터 n_epochs까지\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # data loader가 만드는 batches에서 우리의 dataset을 loop over 함.\n",
        "            \n",
        "            outputs = model(imgs)  # 모델을 통해 batch를 feed\n",
        "            \n",
        "            loss = loss_fn(outputs, labels)  # loss계산\n",
        "\n",
        "            optimizer.zero_grad()  # 이전에서의 gradient제거\n",
        "            \n",
        "            loss.backward()  # backward step 수행\n",
        "            \n",
        "            optimizer.step()  # 모델 업데이트\n",
        "\n",
        "            loss_train += loss.item()  # 전체 loss의 합.\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # batch별 평균 loss 얻기 위해 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6QpszSCEgdz",
        "outputId": "d2859be7-80cc-4d0c-c878-71b98593b0c2"
      },
      "source": [
        "#In [31]:\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)  # cifar2 dataset의 예시로 batches up함. Shuffle로 dataset으로부터 예시의 차원을 랜덤화함.\n",
        "\n",
        "model = Net()  #  네트워크 instantiate\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  확률적 경사 하강법 최적화\n",
        "loss_fn = nn.CrossEntropyLoss()  #  cross entropy loss\n",
        "\n",
        "training_loop(  # 이전에 정의한 training loop를 호출.\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 01:53:29.196615 Epoch 1, Training loss 0.56404867939129\n",
            "2021-07-13 01:53:58.294528 Epoch 10, Training loss 0.32760468742270377\n",
            "2021-07-13 01:54:30.635679 Epoch 20, Training loss 0.30307469284458527\n",
            "2021-07-13 01:55:02.997618 Epoch 30, Training loss 0.28098370514477894\n",
            "2021-07-13 01:55:35.264691 Epoch 40, Training loss 0.26119488620074693\n",
            "2021-07-13 01:56:07.595348 Epoch 50, Training loss 0.24396828690152259\n",
            "2021-07-13 01:56:39.823396 Epoch 60, Training loss 0.22478416324800746\n",
            "2021-07-13 01:57:12.466698 Epoch 70, Training loss 0.2053464343593379\n",
            "2021-07-13 01:57:44.715682 Epoch 80, Training loss 0.18890236574373429\n",
            "2021-07-13 01:58:17.041548 Epoch 90, Training loss 0.17473488685431754\n",
            "2021-07-13 01:58:49.635618 Epoch 100, Training loss 0.1615173067351815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBJaRMzoFdkC",
        "outputId": "4820891e-101a-4d40-fbbe-f363293ecd35"
      },
      "source": [
        "#In [32]:\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # gradient 필요하지 않음, update하지 않아도 됨.\n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # 아웃풋 값으로 가장 큰 인덱스값 내놓음\n",
        "                total += labels.shape[0]  # 예시의 숫자 모두 더함, batch size에 의해 total 값 증가.\n",
        "                correct += int((predicted == labels).sum())  # 예측했던 class(maximum 값)과 ground-truth label비교(Boolean array)\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.94\n",
            "Accuracy val: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOlWmSzXPau-"
      },
      "source": [
        "- error 수 반으로 줄임(validation set에서)\n",
        "- 훨씬 적은 parameters 사용\n",
        "\n",
        "=> locality와 translation invariance를 통해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuReNbOtP1Vp"
      },
      "source": [
        "# 8.4.2 Saving and loading our model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJvS0rHEMU7s"
      },
      "source": [
        "#In [33]:\n",
        "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt') #모델의 모든 파라미터(weights,biases for the two convolution models & two linear models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnER5vjyMXo8",
        "outputId": "d0c370df-fd68-497b-a75e-4b154d01f3ec"
      },
      "source": [
        "#In [34]:\n",
        "loaded_model = Net()  # 저장할 때와 불러올 때 Net()정의를 바꾸지 않도록 주의\n",
        "loaded_model.load_state_dict(torch.load(data_path\n",
        "                                        + 'birds_vs_airplanes.pt'))\n",
        "# 이제 code repository에 미리 훈련된 모델을 포함. 데이터 경로는 'birds_vs_airplanes.pt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_945EAVQoD0"
      },
      "source": [
        "# 8.4.3 Training on the GPU\n",
        "연산을 조금 더 빨리 하기 위해서 GPU에서 연산 수행, tensors(from the data loader) to the GPU, parameters 또한 GPU로 옮길 필요가 있음.\n",
        "\n",
        "다행히도 nn.Module은 .to함수를 수행한다.\n",
        "\n",
        "Module.to/Tensor.to.Module.to <-> Tensor.to 다른 개념.\n",
        "\n",
        "전자는 그 자리에서 tensor를 생성하지만, 후자는 새로운 tensor를 반환한다.\n",
        "(다른 적절한 디바이스에 파라미터를 옮기고 나서 Optimizer를 생성하는 것은 좋음.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qiw62KA6Ma9A",
        "outputId": "16520d0a-b525-43ca-f81d-207d1009f6d7"
      },
      "source": [
        "#In [35]:\n",
        "device = (torch.device('cuda') if torch.cuda.is_available()   # device 변수를 torch.cuda.is_available()에 따라 설정.-> :)\n",
        "          else torch.device('cpu'))\n",
        "print(f\"Training on device {device}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UJezCjGMdaB"
      },
      "source": [
        "#In [36]:\n",
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # imgs와 labels를 우리가 훈련중인 디바이스에 옮기는 것만이 유일한 차이점.\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPqcCigaMfwa",
        "outputId": "7b808a26-e0a7-4e53-fb75-7438e366f1fb"
      },
      "source": [
        "#In [37]:\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)  # 우리의 모델을 GPU로 이동시킴. 빼먹으면 오류 남.(PyTorch는 GPU랑 CPU input mixing 지원 안 함.)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "# validate model에서도 같은 개정이 이루어져야."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 01:59:50.424119 Epoch 1, Training loss 0.5758682179982495\n",
            "2021-07-13 01:59:53.208614 Epoch 10, Training loss 0.3257187461587274\n",
            "2021-07-13 01:59:56.280847 Epoch 20, Training loss 0.2901738556517158\n",
            "2021-07-13 01:59:59.331057 Epoch 30, Training loss 0.2672741515621258\n",
            "2021-07-13 02:00:02.426408 Epoch 40, Training loss 0.24808442118061577\n",
            "2021-07-13 02:00:05.488901 Epoch 50, Training loss 0.22970585311484185\n",
            "2021-07-13 02:00:08.556979 Epoch 60, Training loss 0.2152167026214539\n",
            "2021-07-13 02:00:11.604453 Epoch 70, Training loss 0.19959015842930528\n",
            "2021-07-13 02:00:14.670144 Epoch 80, Training loss 0.18537759090400047\n",
            "2021-07-13 02:00:17.742876 Epoch 90, Training loss 0.16804107401970844\n",
            "2021-07-13 02:00:20.785587 Epoch 100, Training loss 0.15623469520241592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKlCGA9xMiUL",
        "outputId": "095b0020-b818-4d63-c6d9-8e36a80154c3"
      },
      "source": [
        "#In [38]:\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "all_acc_dict = collections.OrderedDict()\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    accdict = {}\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "        accdict[name] = correct / total\n",
        "    return accdict\n",
        "\n",
        "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.92\n",
            "Accuracy val: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "4M615zrTMnGO",
        "outputId": "6e0cdc12-e0d9-483f-ab9f-6324486be3c5"
      },
      "source": [
        "#In [39]:\n",
        "loaded_model = Net().to(device=device)\n",
        "loaded_model.load_state_dict(torch.load(data_path\n",
        "                                        + 'birds_vs_airplanes.pt',\n",
        "                                        map_location=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-92aa0a394a06>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    map_location=device)\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0FmpxRdMoPi"
      },
      "source": [
        "#In [40]:\n",
        "class NetWidth(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 16 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOBCuruMqSg"
      },
      "source": [
        "#In [41]:\n",
        "model = NetWidth().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSeXJ0f4MuUd"
      },
      "source": [
        "#In [42]:\n",
        "class NetWidth(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCUBRScrMwlS"
      },
      "source": [
        "#In [43]:\n",
        "model = NetWidth(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxRyLCkPMy7d"
      },
      "source": [
        "#In [44]:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC78hsDLM1j0"
      },
      "source": [
        "#In [45]:\n",
        "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
        "                        train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            l2_lambda = 0.001\n",
        "            l2_norm = sum(p.pow(2.0).sum()\n",
        "                          for p in model.parameters())  # pow(2.0)을 L1 정규화를 위해 abs()로 대체.\n",
        "            loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_train += loss.item()\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lps3gtw6M34X"
      },
      "source": [
        "#In [46]:\n",
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop_l2reg(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtMm0YDXM58k"
      },
      "source": [
        "#In [47]:\n",
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = self.conv2_dropout(out)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QINP2F4EM8QU"
      },
      "source": [
        "#In [48]:\n",
        "model = NetDropout(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpDUBExTM-DN"
      },
      "source": [
        "#In [49]:\n",
        "class NetBatchNorm(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
        "                               padding=1)\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv1_batchnorm(self.conv1(x))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = self.conv2_batchnorm(self.conv2(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh3X_KsZM__1"
      },
      "source": [
        "#In [50]:\n",
        "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyosY_A6NB2g"
      },
      "source": [
        "#In [51]:\n",
        "class NetDepth(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v674JXiNDh1"
      },
      "source": [
        "#In [52]:\n",
        "model = NetDepth(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7pTmsYhNF8u"
      },
      "source": [
        "#In [53]:\n",
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhGzOYIRNJbD"
      },
      "source": [
        "#In [54]:\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"res\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4BWtvJFNLN6"
      },
      "source": [
        "#In [55]:\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_chans):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
        "                              padding=1, bias=False)  # BatchNorm layer가 bias의 효과를 취소할수 있어서 남겨짐.\n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
        "                                      nonlinearity='relu')  # 표준편차(ResNet paper에서 계산된)와 함께 일반 무작위 요소로 custom 초기화 사용\n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.batch_norm(out)\n",
        "        out = torch.relu(out)\n",
        "        return out + x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLYb9WRKNNFw"
      },
      "source": [
        "#In [56]:\n",
        "class NetResDeep(nn.Module):\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = self.resblocks(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRDQWY3ZNOxp"
      },
      "source": [
        "#In [57]:\n",
        "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "pZabsI1rNRbc",
        "outputId": "3632968b-b1a8-4b7c-f505-c194776277d9"
      },
      "source": [
        "#In [58]:\n",
        "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
        "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
        "\n",
        "width =0.3\n",
        "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
        "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='val')\n",
        "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()),\n",
        "           rotation=60)\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0.7, 1)\n",
        "plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55PazCDUNTrb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}